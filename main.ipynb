{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512d400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdfa4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model_by_video(video_path):\n",
    "\n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils  # Import the drawing utilities\n",
    "\n",
    "    # Load the trained LSTM model\n",
    "    model = load_model('gesture_classification.keras')  # Replace with the path to your trained model\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    landmarks_sequence = []  # List to store sequences of landmarks\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB and process it with MediaPipe Hands\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Convert the NormalizedLandmark objects to numerical representation\n",
    "                landmarks_numeric = np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmarks.landmark])\n",
    "\n",
    "                # Store the hand landmarks in the sequence\n",
    "                landmarks_sequence.append(landmarks_numeric)\n",
    "\n",
    "            # Draw landmarks on the frame\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the frame with or without landmarks\n",
    "        cv2.imshow('Hand Gestures', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    # Assuming max_sequence_length is the desired sequence length\n",
    "    max_sequence_length = 50  # Change this to your desired value - no of frames\n",
    "\n",
    "    # Preprocess the landmarks sequences to have a consistent length\n",
    "    num_landmarks = 21\n",
    "\n",
    "    # Truncate or pad the sequence to match max_sequence_length\n",
    "    if len(landmarks_sequence) >= max_sequence_length:\n",
    "        landmarks_sequence = landmarks_sequence[:max_sequence_length]\n",
    "    else:\n",
    "        num_landmarks = 21\n",
    "        num_coordinates = 3\n",
    "        padding = np.zeros((max_sequence_length - len(landmarks_sequence), num_landmarks, num_coordinates))\n",
    "        landmarks_sequence = np.concatenate((landmarks_sequence, padding), axis=0)\n",
    "\n",
    "    # Convert the landmarks sequence to a numpy array\n",
    "    landmarks_sequence = np.array(landmarks_sequence)\n",
    "\n",
    "    print(landmarks_sequence.shape)\n",
    "\n",
    "    # Assuming all_landmarks_sequences has shape (num_samples, sequence_length, num_landmarks, num_coordinates)\n",
    "    # You need to reshape it to (num_samples, sequence_length, num_landmarks * num_coordinates)\n",
    "    landmarks_sequence = landmarks_sequence.reshape(\n",
    "        1,\n",
    "        landmarks_sequence.shape[0],\n",
    "        landmarks_sequence.shape[1] * landmarks_sequence.shape[2]\n",
    "    )\n",
    "    # landmarks_sequence.shape[0] = frame size\n",
    "    # landmarks_sequence.shape[1] = 21 (no of landmarks of hands)\n",
    "    # landmarks_sequence.shape[2] = x, y, z co-ordinates\n",
    "\n",
    "    # Use the trained model to predict\n",
    "    # Ensure that you preprocess the landmarks_sequence similarly to how you preprocessed during training\n",
    "    # predicted_probs = model.predict(np.expand_dims(landmarks_sequence, axis=0))\n",
    "\n",
    "    print(landmarks_sequence.shape)\n",
    "\n",
    "    predicted_probs = model.predict(landmarks_sequence)\n",
    "    print(predicted_probs)\n",
    "\n",
    "    # Get the class with the highest probability as the predicted gesture\n",
    "    predicted_class = np.argmax(predicted_probs)\n",
    "\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "\n",
    "    # Decode the numeric label back to the original class label\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    print(\"Predicted Gesture:\", predicted_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf52848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model_by_frames(frames):\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils  # Import the drawing utilities\n",
    "    \n",
    "    # Load the trained LSTM model\n",
    "    model = load_model('gesture_classification.keras')\n",
    "\n",
    "    landmarks_sequence = []  # List to store sequences of landmarks\n",
    "\n",
    "    for frame in frames:\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                landmarks_numeric = np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmarks.landmark])\n",
    "                landmarks_sequence.append(landmarks_numeric)\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Display the frame with landmarks\n",
    "        cv2.imshow('Hand Gestures', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    # Assuming max_sequence_length is the desired sequence length\n",
    "    max_sequence_length = 50  # Change this to your desired value - no of frames\n",
    "\n",
    "    # Preprocess the landmarks sequences to have a consistent length\n",
    "    num_landmarks = 21\n",
    "\n",
    "    # Truncate or pad the sequence to match max_sequence_length\n",
    "    if len(landmarks_sequence) >= max_sequence_length:\n",
    "        landmarks_sequence = landmarks_sequence[:max_sequence_length]\n",
    "    else:\n",
    "        num_landmarks = 21\n",
    "        num_coordinates = 3\n",
    "        padding = np.zeros((max_sequence_length - len(landmarks_sequence), num_landmarks, num_coordinates))\n",
    "        landmarks_sequence = np.concatenate((landmarks_sequence, padding), axis=0)\n",
    "\n",
    "    # Convert the landmarks sequence to a numpy array\n",
    "    landmarks_sequence = np.array(landmarks_sequence)\n",
    "\n",
    "    print(landmarks_sequence.shape)\n",
    "\n",
    "    # Assuming all_landmarks_sequences has shape (num_samples, sequence_length, num_landmarks, num_coordinates)\n",
    "    # You need to reshape it to (num_samples, sequence_length, num_landmarks * num_coordinates)\n",
    "    landmarks_sequence = landmarks_sequence.reshape(\n",
    "        1,\n",
    "        landmarks_sequence.shape[0],\n",
    "        landmarks_sequence.shape[1] * landmarks_sequence.shape[2]\n",
    "    )\n",
    "    # landmarks_sequence.shape[0] = frame size\n",
    "    # landmarks_sequence.shape[1] = 21 (no of landmarks of hands)\n",
    "    # landmarks_sequence.shape[2] = x, y, z co-ordinates\n",
    "\n",
    "    # Use the trained model to predict\n",
    "    # Ensure that you preprocess the landmarks_sequence similarly to how you preprocessed during training\n",
    "    # predicted_probs = model.predict(np.expand_dims(landmarks_sequence, axis=0))\n",
    "\n",
    "    print(landmarks_sequence.shape)\n",
    "\n",
    "    predicted_probs = model.predict(landmarks_sequence)\n",
    "    print(predicted_probs)\n",
    "\n",
    "    # Get the class with the highest probability as the predicted gesture\n",
    "    predicted_class = np.argmax(predicted_probs)\n",
    "\n",
    "    label_encoder = joblib.load('label_encoder.joblib')\n",
    "\n",
    "    # Decode the numeric label back to the original class label\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    print(\"Predicted Gesture:\", predicted_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e124d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the segmented videos\n",
    "\n",
    "# Rum time - 62 sec\n",
    "\n",
    "# Initialize Mediapipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load the input video\n",
    "video_path = 'multiclass.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create a folder for saving gesture videos\n",
    "output_folder = 'gestures'\n",
    "\n",
    "if os.path.exists(output_folder):\n",
    "    try:\n",
    "        shutil.rmtree(output_folder)\n",
    "        print(f\"Contents of '{output_folder}' removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing contents of '{output_folder}': {e}\")\n",
    "else:\n",
    "    print(f\"'{output_folder}' does not exist.\")\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Initialize variables for gesture segmentation\n",
    "frame_buffer = []\n",
    "gesture_frames = []\n",
    "gesture_detected = False\n",
    "min_gesture_frames = 30  # Minimum number of frames to consider as a gesture\n",
    "gesture_count = 0\n",
    "\n",
    "# Output video writer\n",
    "output_video_writer = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR frame to RGB for Mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect hands in the frame\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        if not gesture_detected:\n",
    "            gesture_detected = True\n",
    "        frame_buffer.append(frame)\n",
    "    else:\n",
    "        if gesture_detected and len(frame_buffer) >= min_gesture_frames:\n",
    "            gesture_frames = frame_buffer.copy()\n",
    "\n",
    "            # Save the gesture_frames as a separate video\n",
    "            gesture_count += 1\n",
    "            output_path = os.path.join(output_folder, f'gesture_{gesture_count}.avi')\n",
    "            if output_video_writer is None:\n",
    "                height, width, _ = gesture_frames[0].shape\n",
    "                output_video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (width, height))\n",
    "            for gesture_frame in gesture_frames:\n",
    "                output_video_writer.write(gesture_frame)\n",
    "\n",
    "            # Release the output video writer\n",
    "            output_video_writer.release()\n",
    "            output_video_writer = None\n",
    "\n",
    "        gesture_detected = False\n",
    "        frame_buffer = []\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06dc2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent threads for video writting and frames extraction\n",
    "\n",
    "# Run time - 60 sec\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import concurrent.futures\n",
    "\n",
    "# Initialize Mediapipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load the input video\n",
    "video_path = 'multiclass.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create a folder for saving gesture videos\n",
    "output_folder = 'gestures'\n",
    "\n",
    "if os.path.exists(output_folder):\n",
    "    try:\n",
    "        shutil.rmtree(output_folder)\n",
    "        print(f\"Contents of '{output_folder}' removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing contents of '{output_folder}': {e}\")\n",
    "else:\n",
    "    print(f\"'{output_folder}' does not exist.\")\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Initialize variables for gesture segmentation\n",
    "frame_indices = []\n",
    "gesture_detected = False\n",
    "min_gesture_frames = 30  # Minimum number of frames to consider as a gesture\n",
    "\n",
    "# Output video writers\n",
    "output_video_writers = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR frame to RGB for Mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect hands in the frame\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        if not gesture_detected:\n",
    "            gesture_detected = True\n",
    "            start_frame = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "    else:\n",
    "        if gesture_detected:\n",
    "            end_frame = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "            num_frames = end_frame - start_frame\n",
    "            if num_frames >= min_gesture_frames:\n",
    "                frame_indices.append((int(start_frame), int(end_frame)))\n",
    "            gesture_detected = False\n",
    "\n",
    "# Process gesture frames concurrently\n",
    "def process_gesture(gesture_frames, output_path):\n",
    "    height, width, _ = gesture_frames[0].shape\n",
    "    output_video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (width, height))\n",
    "    for gesture_frame in gesture_frames:\n",
    "        output_video_writer.write(gesture_frame)\n",
    "    output_video_writer.release()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for start_frame, end_frame in frame_indices:\n",
    "        gesture_frames = []\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        for _ in range(end_frame - start_frame + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            gesture_frames.append(frame)\n",
    "        output_path = os.path.join(output_folder, f'gesture_{len(futures) + 1}.avi')\n",
    "        futures.append(executor.submit(process_gesture, gesture_frames, output_path))\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70355fdd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gesture_1.avi\n",
      "(50, 21, 3)\n",
      "(1, 50, 63)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[[0.00959661 0.00557783 0.00600773 0.00147017 0.8236088  0.00692855\n",
      "  0.13257937 0.01423097]]\n",
      "Predicted Gesture: hot\n",
      "gesture_2.avi\n",
      "(50, 21, 3)\n",
      "(1, 50, 63)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.03237595 0.30407634 0.09674851 0.03742347 0.08001267 0.15176386\n",
      "  0.2673899  0.03020922]]\n",
      "Predicted Gesture: call\n",
      "gesture_3.avi\n",
      "(50, 21, 3)\n",
      "(1, 50, 63)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.01526649 0.51758665 0.0360455  0.01415846 0.03755576 0.06115115\n",
      "  0.29816326 0.02007264]]\n",
      "Predicted Gesture: call\n",
      "gesture_4.avi\n",
      "(50, 21, 3)\n",
      "(1, 50, 63)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.63244915 0.02498532 0.03682843 0.05022684 0.04561914 0.02656052\n",
      "  0.02094135 0.16238932]]\n",
      "Predicted Gesture: accident\n",
      "thief_001_02.AVI\n",
      "(50, 21, 3)\n",
      "(1, 50, 63)\n",
      "1/1 [==============================] - 1s 781ms/step\n",
      "[[0.05628915 0.022482   0.00686561 0.00338208 0.3042078  0.00885591\n",
      "  0.02008308 0.5778343 ]]\n",
      "Predicted Gesture: thief\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_paths = os.listdir('gestures')\n",
    "for video in video_paths:\n",
    "    print(video)\n",
    "    run_model_by_video('gestures/'+video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "610ffaa4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 21, 3)\n",
      "(1, 50, 63)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EDBD1F71C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.63244915 0.02498532 0.03682843 0.05022684 0.04561914 0.02656052\n",
      "  0.02094135 0.16238932]]\n",
      "Predicted Gesture: accident\n"
     ]
    }
   ],
   "source": [
    "run_model_by_video('gestures/gesture_4.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf8ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d55e012",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m gesture_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 22\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the frames of segmented gestures and get those landmarks\n",
    "\n",
    "# Run time - 50 sec\n",
    "\n",
    "# Initialize Mediapipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load the input video\n",
    "video_path = 'multiclass.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize variables for gesture segmentation\n",
    "frame_buffer = []\n",
    "gesture_frames = []\n",
    "gesture_detected = False\n",
    "min_gesture_frames = 30  # Minimum number of frames to consider as a gesture\n",
    "gesture_count = 0\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR frame to RGB for Mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect hands in the frame\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        if not gesture_detected:\n",
    "            gesture_detected = True\n",
    "        frame_buffer.append(frame)\n",
    "    else:\n",
    "        if gesture_detected and len(frame_buffer) >= min_gesture_frames:\n",
    "            gesture_frames = frame_buffer.copy()  # one gesture video\n",
    "            run_model_by_frames(gesture_frames)\n",
    "\n",
    "\n",
    "        gesture_detected = False\n",
    "        frame_buffer = []\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66caeebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
